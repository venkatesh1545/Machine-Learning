{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOiG9kC1rdqvZ34TcXz0kSN"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9WKWa0QIjCAC","executionInfo":{"status":"ok","timestamp":1740427617697,"user_tz":-330,"elapsed":4130,"user":{"displayName":"venkatesh golthi","userId":"14224435033662974193"}},"outputId":"733c7788-c62d-43a5-978b-823a1ee5a840"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n","Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.1.8)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.4.2)\n","Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk) (4.67.1)\n"]}],"source":["!pip install nltk"]},{"cell_type":"code","source":["corpus = \"\"\"\n","  Hello Welcome to NLP(Natural Language processing. In this iam learning about Tokenizattion)\n","\n","\"\"\""],"metadata":{"id":"Komm-pR5jgEp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import nltk\n","nltk.download('punkt_tab')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2xvoaMENkqMP","executionInfo":{"status":"ok","timestamp":1740427680913,"user_tz":-330,"elapsed":510,"user":{"displayName":"venkatesh golthi","userId":"14224435033662974193"}},"outputId":"f0784796-1bfa-4839-9929-a11caad7edaa"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":13}]},{"cell_type":"code","source":["### paragraph --> sentence.\n","from nltk.tokenize import sent_tokenize\n","sent_tokenize(corpus)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Ik5vfAtuj47w","executionInfo":{"status":"ok","timestamp":1740427684857,"user_tz":-330,"elapsed":182,"user":{"displayName":"venkatesh golthi","userId":"14224435033662974193"}},"outputId":"5081ad5e-95cb-4a97-9ba4-16a238aaea0d"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['\\n  Hello Welcome to NLP(Natural Language processing.',\n"," 'In this iam learning about Tokenizattion)']"]},"metadata":{},"execution_count":14}]},{"cell_type":"code","source":["### paragraph --> words\n","from nltk.tokenize import word_tokenize\n","word_tokenize(corpus)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"21LhhN8UkNVE","executionInfo":{"status":"ok","timestamp":1740428083570,"user_tz":-330,"elapsed":63,"user":{"displayName":"venkatesh golthi","userId":"14224435033662974193"}},"outputId":"cb63420f-4d6b-4142-fc1f-f715386fe549"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['Hello',\n"," 'Welcome',\n"," 'to',\n"," 'NLP',\n"," '(',\n"," 'Natural',\n"," 'Language',\n"," 'processing',\n"," '.',\n"," 'In',\n"," 'this',\n"," 'iam',\n"," 'learning',\n"," 'about',\n"," 'Tokenizattion',\n"," ')']"]},"metadata":{},"execution_count":19}]},{"cell_type":"code","source":[" ## this makes each character into seperate string.\n"," from nltk.tokenize import wordpunct_tokenize\n"," wordpunct_tokenize(corpus)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"W8h2w8iZlcZb","executionInfo":{"status":"ok","timestamp":1740428093394,"user_tz":-330,"elapsed":20,"user":{"displayName":"venkatesh golthi","userId":"14224435033662974193"}},"outputId":"908ad943-cd8c-470b-c6d7-2de10551cf15"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['Hello',\n"," 'Welcome',\n"," 'to',\n"," 'NLP',\n"," '(',\n"," 'Natural',\n"," 'Language',\n"," 'processing',\n"," '.',\n"," 'In',\n"," 'this',\n"," 'iam',\n"," 'learning',\n"," 'about',\n"," 'Tokenizattion',\n"," ')']"]},"metadata":{},"execution_count":20}]},{"cell_type":"code","source":["from nltk.tokenize import TreebankWordTokenizer\n","tokenizer = TreebankWordTokenizer()\n","tokenizer.tokenize(corpus)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"k0fm9_xMmAzD","executionInfo":{"status":"ok","timestamp":1740428280085,"user_tz":-330,"elapsed":25,"user":{"displayName":"venkatesh golthi","userId":"14224435033662974193"}},"outputId":"8fe5b36b-b7bd-4344-a3b8-d091b4874196"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['Hello',\n"," 'Welcome',\n"," 'to',\n"," 'NLP',\n"," '(',\n"," 'Natural',\n"," 'Language',\n"," 'processing.',\n"," 'In',\n"," 'this',\n"," 'iam',\n"," 'learning',\n"," 'about',\n"," 'Tokenizattion',\n"," ')']"]},"metadata":{},"execution_count":22}]},{"cell_type":"code","source":[],"metadata":{"id":"mJt4_l4Nmmz3"},"execution_count":null,"outputs":[]}]}